# 创建用户
```sh
useradd hadoop
passwd hadoop
hadooppwd
```

# core-site.xml

```xml

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop001:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/datahadoop</value>
    </property>
</configuration>

```

# hdfs-site.xml

```xml

<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop002:50090</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

```

# yarn-site.xml

```xml

<configuration>

	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>hadoop001</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>  
		<name>yarn.log-aggregation-enable</name>  
		<value>true</value>  
	</property> 
    
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>8192</value>
        <!--表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节
点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。 -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>256</value>
        <!--单个任务可申请的最少物理内存量，默认是1024（MB），如果一个任务申请的物理
内存量少于该值，则该对应的值改为这个数。 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>8192</value>
        <!--单个任务可申请的最多物理内存量，默认是8192（MB）。 -->
    </property>
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
        <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 -->
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
        <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 -->
    </property>

    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>8</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>4</value>
    </property>
	
</configuration>

```

# mapred-site.xml, hadoop3.x

```xml

<configuration>
	<property>
		<name>yarn.app.mapreduce.am.env</name>
		<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
	<property>
		<name>mapreduce.map.env</name>
		<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
	<property>
		<name>mapreduce.reduce.env</name>
		<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
	</property>
</configuration>

```

# mapred-site.xml, hadoop2.x
```xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop001:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop001:19888</value>
    </property>
</configuration>

```

```sh
ssh-keygen -t rsa
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop001
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop002
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@hadoop003
```


# 拷贝安装文件, 环境变量配置文件
```sh
scp -r *hadoop* .bash_profile hadoop002:/home/hadoop/
scp -r *hadoop* .bash_profile hadoop003:/home/hadoop/

scp -r hadoop-2.9.2/etc/ hadoop002:/home/hadoop/hadoop-2.9.2/
scp -r hadoop-2.9.2/etc/ hadoop003:/home/hadoop/hadoop-2.9.2/
```

# 加载环境变量配置文件
```sh
source .bash_profile
```

# 查看hadoop版本
```sh
hadoop version
```

# 初始化namenode
```sh
hdfs namenode -format
```
# 启动集群
```sh
start-dfs.sh
start-yarn.sh
mapred --daemon start historyserver
```

# hadoop3.x  节点文件变更为workers
Slaves File
List all worker hostnames or IP addresses in your etc/hadoop/workers file, one per line. Helper scripts (described below) will 
use the etc/hadoop/workers file to run commands on many hosts at once. It is not used for any of the Java-based Hadoop configuration. 
In order to use this functionality, ssh trusts (via either passphraseless ssh or some other means, such as Kerberos) must be 
established for the accounts used to run Hadoop.

# HDFS集群UI
http://hadoop001:9870

# YARN集群UI
http://hadoop001:8088

# 测试
```sh
hadoop jar /home/hadoop/hadoop-3.1.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.2.jar wordcount a.txt output

hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount a.txt output
```

# 检查hadoop支持的压缩方式
```sh
hadoop checknative -a
```

```sh
mvn clean package -DskipTests -Pdist,native -Dtar -Dsnappy.lib=/usr/local/lib -Dbundle.snappy
```
